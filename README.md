# Transformer_Math
Project work for the exam of deep learning at the master's course in Ai at the university of Bologna. The task involves training a neural network at translating mathematical expressions from infix notation to Reverse Polish Notation (RPN), which i solved using a Transformer-based architecture.
The project is limited at expressions of depth 3.
## Future works
The model could be trained on longer sequences, seeing how a little sized model would learn harder and longer order patterns (depth 4,5,6...).
